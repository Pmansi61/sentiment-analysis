# -*- coding: utf-8 -*-
"""Roberta.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lO8h7PN6enJdNbsDznX5qA1SPiajt3GV
"""

# Import PyDrive and associated libraries
# This only needs to be done once per notebook
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
# This only needs to be done once per notebook
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Download a file based on its file ID.

# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz
file_id = '1EsPXltlynyR-SvoOmi7PM43lVAQcw_tI' # Check your own ID in GDrive
downloaded = drive.CreateFile({'id': file_id})

# Save file in Colab memory
downloaded.GetContentFile('Dataset_V4.csv')

import pandas as pd

# Load your dataset
df = pd.read_csv('Dataset_V4.csv')

# Inspect the dataset
print(df.head())

# Split the single column into three separate columns
df[['text_id', 'tweet_text', 'sentiment']] = df['text_id;tweet_text;sentiment'].str.split(';', expand=True)

# Drop the original concatenated column
df.drop('text_id;tweet_text;sentiment', axis=1, inplace=True)

# Display the updated DataFrame
df.info()
df.head()

from sklearn.model_selection import train_test_split

# Split the dataset into training and testing sets
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)

# Print the shapes of the training and testing sets
print("Training set shape:", train_data.shape)
print("Testing set shape:", test_data.shape)

# Import necessary libraries for RoBERTa
from transformers import RobertaTokenizer
from torch.utils.data import DataLoader, TensorDataset
import torch

# Tokenize and preprocess text using RoBERTa
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

def tokenize_text_roberta(text):
    return tokenizer.encode_plus(
        text,
        max_length=128,
        add_special_tokens=True,
        return_token_type_ids=False,
        padding='max_length',
        return_attention_mask=True,
        return_tensors='pt'
    )

# Tokenize and preprocess the training data for RoBERTa
train_texts = train_data['tweet_text'].tolist()
train_labels = train_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).tolist()

tokenized_train_data = [tokenize_text_roberta(text) for text in train_texts]
input_ids = torch.cat([item['input_ids'] for item in tokenized_train_data], dim=0)
attention_masks = torch.cat([item['attention_mask'] for item in tokenized_train_data], dim=0)
labels = torch.tensor(train_labels)

# Create a TensorDataset for RoBERTa
train_dataset = TensorDataset(input_ids, attention_masks, labels)

# Fine-tune the RoBERTa model
from transformers import RobertaForSequenceClassification
from torch import optim
from transformers import AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm

model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)
optimizer_roberta = AdamW(model_roberta.parameters(), lr=2e-5)
scheduler_roberta = get_linear_schedule_with_warmup(optimizer_roberta, num_warmup_steps=0, num_training_steps=len(train_dataset))

# Set up GPU acceleration if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_roberta.to(device)

# Training loop for RoBERTa
from torch import nn

epochs = 3

# Create a DataLoader for training data
train_dataloader_roberta = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Set the RoBERTa model to training mode
model_roberta.train()

# Define the loss function
criterion_roberta = nn.CrossEntropyLoss()

# Training loop for RoBERTa
for epoch in range(epochs):
    total_loss = 0.0
    progress_bar = tqdm(train_dataloader_roberta, desc=f'Epoch {epoch + 1}/{epochs}')

    for batch in progress_bar:
        # Move the batch to the GPU if available
        inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}

        # Zero the gradients
        optimizer_roberta.zero_grad()

        # Forward pass
        outputs = model_roberta(**inputs)
        loss = criterion_roberta(outputs.logits, inputs['labels'])

        # Backward pass and optimization
        loss.backward()
        optimizer_roberta.step()
        scheduler_roberta.step()

        total_loss += loss.item()
        progress_bar.set_postfix({'Training Loss': total_loss / len(train_dataloader_roberta)})

# Tokenize and preprocess the testing data using RoBERTa
test_texts_roberta = test_data['tweet_text'].tolist()
test_labels_roberta = test_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).tolist()

tokenized_test_data_roberta = [tokenize_text_roberta(text) for text in test_texts_roberta]
input_ids_test_roberta = torch.cat([item['input_ids'] for item in tokenized_test_data_roberta], dim=0)
attention_masks_test_roberta = torch.cat([item['attention_mask'] for item in tokenized_test_data_roberta], dim=0)
labels_test_roberta = torch.tensor(test_labels_roberta)

# Create a TensorDataset for testing with RoBERTa
test_dataset_roberta = TensorDataset(input_ids_test_roberta, attention_masks_test_roberta, labels_test_roberta)

# Create a DataLoader for testing data with RoBERTa
test_dataloader_roberta = DataLoader(test_dataset_roberta, batch_size=32, shuffle=False)

import time
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc
import seaborn as sns
import matplotlib.pyplot as plt

# Evaluation for RoBERTa
model_roberta.eval()

all_predictions_roberta = []
all_labels_roberta = []

start_time_roberta = time.time()

with torch.no_grad():
    for batch in test_dataloader_roberta:
        inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}

        outputs = model_roberta(**inputs)

        predictions_roberta = torch.argmax(outputs.logits, dim=1)

        all_predictions_roberta.extend(predictions_roberta.cpu().numpy())
        all_labels_roberta.extend(inputs['labels'].cpu().numpy())

# Calculate metrics for RoBERTa
accuracy_roberta = accuracy_score(all_labels_roberta, all_predictions_roberta)
precision_roberta, recall_roberta, f1_roberta, _ = precision_recall_fscore_support(all_labels_roberta, all_predictions_roberta, average='binary')
conf_matrix_roberta = confusion_matrix(all_labels_roberta, all_predictions_roberta)

# ROC Curve for RoBERTa
fpr_roberta, tpr_roberta, thresholds_roberta = roc_curve(all_labels_roberta, all_predictions_roberta)
roc_auc_roberta = auc(fpr_roberta, tpr_roberta)

# Plot ROC Curve for RoBERTa
plt.figure(figsize=(8, 8))
plt.plot(fpr_roberta, tpr_roberta, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_roberta:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for RoBERTa')
plt.legend(loc='lower right')
plt.show()

# Visualize Confusion Matrix for RoBERTa
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_roberta, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for RoBERTa')
plt.show()

# Calculate total time for inference for RoBERTa
total_time_roberta = time.time() - start_time_roberta

# Function to calculate time taken in seconds for RoBERTa
def time_taken_roberta(start_time):
    return time.time() - start_time

# Print metrics and confusion matrix for RoBERTa
print(f'Testing Accuracy for RoBERTa: {accuracy_roberta * 100:.2f}%')
print(f'Precision for RoBERTa: {precision_roberta:.2f}')
print(f'Recall for RoBERTa: {recall_roberta:.2f}')
print(f'F1 Score for RoBERTa: {f1_roberta:.2f}')
print(f'Total Inference Time for RoBERTa: {total_time_roberta:.2f} seconds')
print('Confusion Matrix for RoBERTa:')
print(conf_matrix_roberta)

# Time taken in seconds for RoBERTa
print(f'Time taken for RoBERTa: {time_taken_roberta(start_time_roberta):.2f} seconds')