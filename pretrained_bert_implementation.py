# -*- coding: utf-8 -*-
"""PreTrained_BERT Implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Al3a47s173t8rm7nQfKky0oq_P2-7HVz
"""

# Import PyDrive and associated libraries
# This only needs to be done once per notebook
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
# This only needs to be done once per notebook
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Download a file based on its file ID.

# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz
file_id = '1ZnDlLXWhf1iN9Zc9sfpnhQOuUdc1AJ4t' # Check your own ID in GDrive
downloaded = drive.CreateFile({'id': file_id})

# Save file in Colab memory
downloaded.GetContentFile('processed_text.csv')

!pip install transformers
!pip install torch
!pip install nltk
!pip install vaderSentiment

import pandas as pd

# Load your dataset
df = pd.read_csv('processed_text.csv')

# Inspect the dataset
print(df.head())

# Display the information about the DataFrame
df.info()

# Display the first few rows of the DataFrame
df.head()

# Drop the original concatenated column
df.drop('tweet_text', axis=1, inplace=True)

# Display the updated DataFrame
df.info()
df.head()

from sklearn.model_selection import train_test_split

# Split the dataset into training and testing sets
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)

# Print the shapes of the training and testing sets
print("Training set shape:", train_data.shape)
print("Testing set shape:", test_data.shape)

# Tokenize and preprocess the testing data
test_texts = test_data['processed_text'].tolist()
test_labels = test_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).tolist()

tokenized_test_data = [tokenize_text(text) for text in test_texts]
input_ids_test = torch.cat([item['input_ids'] for item in tokenized_test_data], dim=0)
attention_masks_test = torch.cat([item['attention_mask'] for item in tokenized_test_data], dim=0)
labels_test = torch.tensor(test_labels)

# Create a TensorDataset for testing
test_dataset = TensorDataset(input_ids_test, attention_masks_test, labels_test)

# Create a DataLoader for testing data
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

import torch

!pip install transformers
import torch
from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, TensorDataset

# Tokenize and preprocess text
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_text(text):
    return tokenizer.encode_plus(
        text,
        max_length=128,
        add_special_tokens=True,
        return_token_type_ids=False,
        padding='max_length',
        return_attention_mask=True,
        return_tensors='pt'
    )
# Filter out missing values
train_data = train_data.dropna(subset=['processed_text'])
# Tokenize and preprocess the training data
train_texts = train_data['processed_text'].tolist()
train_labels = train_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).tolist()

tokenized_train_data = [tokenize_text(text) for text in train_texts]
input_ids = torch.cat([item['input_ids'] for item in tokenized_train_data], dim=0)
attention_masks = torch.cat([item['attention_mask'] for item in tokenized_train_data], dim=0)
labels = torch.tensor(train_labels)

# Create a TensorDataset
train_dataset = TensorDataset(input_ids, attention_masks, labels)

# Fine-tune the BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
optimizer = AdamW(model.parameters(), lr=2e-5)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataset))

# Set up GPU acceleration if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

from torch.utils.data import DataLoader, random_split
from tqdm.notebook import tqdm

# Training loop
epochs = 3  # You may adjust the number of training epochs based on your specific dataset and task

# Create a DataLoader for training data
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Set the model to training mode
model.train()

# Define the loss function
criterion = torch.nn.CrossEntropyLoss()

# Training loop
for epoch in range(epochs):
    total_loss = 0.0
    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}')

    for batch in progress_bar:
        # Move the batch to the GPU if available
        inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(**inputs)
        loss = criterion(outputs.logits, inputs['labels'])

        # Backward pass and optimization
        loss.backward()
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()
        progress_bar.set_postfix({'Training Loss': total_loss / len(train_dataloader)})

pip install scikit-learn seaborn

from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc
import seaborn as sns
import matplotlib.pyplot as plt
import time

# Set the model to evaluation mode
model.eval()

# Lists to store predictions and labels
all_predictions = []
all_labels = []

# Timing the inference
start_time = time.time()

with torch.no_grad():
    for batch in test_dataloader:
        # Move the batch to the GPU if available
        inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}

        # Forward pass
        outputs = model(**inputs)

        # Get predictions
        predictions = torch.argmax(outputs.logits, dim=1)

        # Append to lists
        all_predictions.extend(predictions.cpu().numpy())
        all_labels.extend(inputs['labels'].cpu().numpy())

# Calculate metrics
accuracy = accuracy_score(all_labels, all_predictions)
precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')
conf_matrix = confusion_matrix(all_labels, all_predictions)

# ROC Curve
fpr, tpr, thresholds = roc_curve(all_labels, all_predictions)
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure(figsize=(8, 8))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

# Visualize Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Calculate total time for inference
total_time = time.time() - start_time

# Function to calculate time taken in seconds
def time_taken(start_time):
    return time.time() - start_time

# Print metrics and confusion matrix
print(f'Testing Accuracy: {accuracy * 100:.2f}%')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')
print(f'Total Inference Time: {total_time:.2f} seconds')
print('Confusion Matrix:')
print(conf_matrix)

# Time taken in seconds
print(f'Time taken: {time_taken(start_time):.2f} seconds')