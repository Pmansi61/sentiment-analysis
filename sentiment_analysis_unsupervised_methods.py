# -*- coding: utf-8 -*-
"""Sentiment Analysis Unsupervised methods.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V4p4rYOCgib5wx6vx-i5e0hKrMlGh2xE
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# Import PyDrive2 and associated libraries
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive2 client
# This only needs to be done once per notebook
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

import pandas as pd
import numpy as np

# Download a file based on its file ID.

# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz
file_id = '1EsPXltlynyR-SvoOmi7PM43lVAQcw_tI' # Check your own ID in GDrive
downloaded = drive.CreateFile({'id': file_id})

# Save file in Colab memory
downloaded.GetContentFile('Dataset_V4.csv')

df = pd.read_csv("Dataset_V4.csv")

df.sample(20)

#Since the data is imported as a string , we would break it in three different columns
df[['tweet_id', 'tweet_text', 'sentiment']] = df['text_id;tweet_text;sentiment'].str.split(pat=';', n=2, expand=True)

df = df.drop(columns=['text_id;tweet_text;sentiment'])

df = df.drop(columns=['sentiment'])

df.sample(20)

import regex as re

def replace_user(tweet, default_replace="tweetuser"):
  tweet = re.sub('\B@\w+', default_replace, tweet)
  return tweet

def replace_url(tweet, default_replace=""):
  tweet = re.sub('(http|https):\/\/\S+', default_replace, tweet)
  return tweet

def replace_hashtag(tweet, default_replace=""):
  tweet = re.sub('#+', default_replace, tweet)
  return tweet

def to_lowercase(tweet):
  tweet = tweet.lower()
  return tweet

def word_repetition(tweet):
  tweet = re.sub(r'(.)\1+', r'\1\1', tweet)
  return tweet

def punct_repetition(tweet, default_replace=""):
  tweet = re.sub(r'[\?\.\!]+(?=[\?\.\!])', default_replace, tweet)
  return tweet

pip install contractions

import contractions

print(contractions.contractions_dict)

def fix_contractions(tweet):
  tweet = contractions.fix(tweet)
  return tweet

pip install nltk

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

def tokenize(tweet):
  tokens = word_tokenize(tweet)
  return tokens

import string

print(string.punctuation)

from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
print(stop_words)

stop_words.discard('not')
stop_words.discard('no')
print(stop_words)

def custom_tokenize(tweet,
                    keep_punct=False,
                    keep_alnum=False,
                    keep_stop=False):

  token_list = word_tokenize(tweet)

  if not keep_punct:
    token_list = [token for token in token_list
                  if token not in string.punctuation]

  if not keep_alnum:
    token_list = [token for token in token_list if token.isalpha()]

  if not keep_stop:
    stop_words = set(stopwords.words('english'))
    stop_words.discard('not')
    token_list = [token for token in token_list if not token in stop_words]

  return token_list

from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer
from nltk.stem.snowball import SnowballStemmer

porter_stemmer = PorterStemmer()
lancaster_stemmer = LancasterStemmer()
snoball_stemmer = SnowballStemmer('english')

def stem_tokens(tokens, stemmer):
  token_list = []
  for token in tokens:
    token_list.append(stemmer.stem(token))
  return token_list

def process_tweet(tweet, verbose=False):
  if verbose: print("Initial tweet: {}".format(tweet))

  ## Twitter Features
  tweet = replace_user(tweet, "") # replace user tag
  tweet = replace_url(tweet)    # replace url
  tweet = replace_hashtag(tweet) # replace hashtag
  if verbose: print("Post Twitter processing tweet: {}".format(tweet))

  ## Word Features
  tweet = to_lowercase(tweet) # lower case
  tweet = fix_contractions(tweet) # replace contractions
  tweet = punct_repetition(tweet) # replace punctuation repetition
  tweet = word_repetition(tweet)# replace word repetition
  if verbose: print("Post Word processing tweet: {}".format(tweet))

  ## Tokenization & Stemming
  tokens = custom_tokenize(tweet, keep_alnum=False, keep_stop=False) # tokenize
  stemmer = SnowballStemmer("english") # define stemmer
  stem = stem_tokens(tokens, stemmer)# stem tokens

  return stem

import random

for i in range(5):
  tweet_id = random.randint(0,len(df))
  tweet = df.iloc[tweet_id]["tweet_text"]
  print(process_tweet(tweet, verbose=True))
  print("\n")

# Assuming df is your DataFrame with the 'tweet_text' column
df['tokens'] = df['tweet_text'].apply(process_tweet)

# Display a sample of the tokenized tweets
print(df[['tweet_text', 'tokens']].sample(10))

import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

# Prepare the processed text
df['processed_text'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))

# Set max_features based on your previous experiments
max_features = 2000

# TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=max_features)
X_tfidf = tfidf_vectorizer.fit_transform(df['processed_text'])

# BoW Vectorization
bow_vectorizer = CountVectorizer(max_features=max_features)
X_bow = bow_vectorizer.fit_transform(df['processed_text'])

# Function to find optimal number of clusters and plot the results
def find_optimal_clusters_and_plot(X, max_k, description):
    silhouette_scores = []
    k_values = range(2, max_k + 1)

    for k in k_values:
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(X)
        score = silhouette_score(X, labels)
        silhouette_scores.append(score)
        print(f"Silhouette Score for {k} clusters: {score}")

    # Plotting the silhouette scores
    plt.figure(figsize=(10, 6))
    plt.plot(k_values, silhouette_scores, 'bx-')
    plt.xlabel('Number of clusters')
    plt.ylabel('Silhouette Score')
    plt.title(f'Silhouette Scores for various numbers of clusters - {description}')
    plt.show()

    optimal_k = k_values[silhouette_scores.index(max(silhouette_scores))]
    return optimal_k

# Find and plot optimal number of clusters for TF-IDF
optimal_k_tfidf = find_optimal_clusters_and_plot(X_tfidf, 10, 'TF-IDF')
print(f"Optimal number of clusters for TF-IDF: {optimal_k_tfidf}")

# Find and plot optimal number of clusters for BoW
optimal_k_bow = find_optimal_clusters_and_plot(X_bow, 10, 'BoW')
print(f"Optimal number of clusters for BoW: {optimal_k_bow}")

from sklearn.decomposition import PCA
import time
# Choose the number of PCA components
n_components = 100

# TF-IDF PCA
pca_tfidf = PCA(n_components=n_components)
X_tfidf_pca = pca_tfidf.fit_transform(X_tfidf.toarray())

# BoW PCA
pca_bow = PCA(n_components=n_components)
X_bow_pca = pca_bow.fit_transform(X_bow.toarray())

# Function to apply K-means and evaluate with silhouette score
def apply_kmeans_and_evaluate(X, num_clusters, description):
    start_time = time.time()
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    kmeans.fit(X)
    end_time = time.time()

    labels = kmeans.labels_
    silhouette_avg = silhouette_score(X, labels)
    print(f"Silhouette Score for {num_clusters} clusters with {description}: {silhouette_avg}")
    print(f"Time taken for {description}: {end_time - start_time:.2f} seconds")
    return kmeans, labels

# Using the optimal number of clusters determined for TF-IDF
kmeans_tfidf_pca, labels_tfidf_pca = apply_kmeans_and_evaluate(X_tfidf_pca, optimal_k_tfidf, 'TF-IDF PCA')
df['tfidf_pca_cluster_labels'] = labels_tfidf_pca

# Using the optimal number of clusters determined for BoW
kmeans_bow_pca, labels_bow_pca = apply_kmeans_and_evaluate(X_bow_pca, optimal_k_bow, 'BoW PCA')
df['bow_pca_cluster_labels'] = labels_bow_pca

import matplotlib.pyplot as plt

def plot_cluster_centroids(X_pca, kmeans_model, title):
    # Extract the cluster centroids from the KMeans model
    centroids = kmeans_model.cluster_centers_

    # Plot the PCA-reduced data points
    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_model.labels_, cmap='viridis', marker='o', alpha=0.6, label='Data points')

    # Plot the centroids
    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=100, label='Centroids')

    plt.title(title)
    plt.xlabel('PCA Feature 1')
    plt.ylabel('PCA Feature 2')
    plt.legend()
    plt.show()

# Plot for TF-IDF K-means Clusters
plot_cluster_centroids(X_tfidf_pca, kmeans_tfidf_pca, 'Cluster Centroids for TF-IDF K-means')

# Plot for BoW K-means Clusters
plot_cluster_centroids(X_bow_pca, kmeans_bow_pca, 'Cluster Centroids for BoW K-means')

from collections import Counter

# Function to get top terms per cluster
def get_top_terms_per_cluster(kmeans_model, vectorizer, texts, n_terms=10):
    # Create a reverse lookup for feature index to actual term in the vocabulary
    feature_names = vectorizer.get_feature_names_out()

    # Get the centroids of the clusters
    ordered_centroids = kmeans_model.cluster_centers_.argsort()[:, ::-1]

    # Dictionary to hold the top terms for each cluster
    terms_per_cluster = {}

    # For each cluster
    for cluster_num in range(kmeans_model.n_clusters):
        # Get the top feature indices for that cluster
        top_feature_indices = ordered_centroids[cluster_num, :n_terms]

        # Map indices to terms
        top_terms = [feature_names[i] for i in top_feature_indices]

        # Assign top terms to the cluster number in the dictionary
        terms_per_cluster[cluster_num] = top_terms

    return terms_per_cluster

# For TF-IDF clusters
top_terms_tfidf = get_top_terms_per_cluster(kmeans_tfidf_pca, tfidf_vectorizer, df['processed_text'])
print("Top terms per cluster for TF-IDF:\n")
for cluster, terms in top_terms_tfidf.items():
    print(f"Cluster {cluster}: {terms}\n")

# For BoW clusters
top_terms_bow = get_top_terms_per_cluster(kmeans_bow_pca, bow_vectorizer, df['processed_text'])
print("Top terms per cluster for BoW:\n")
for cluster, terms in top_terms_bow.items():
    print(f"Cluster {cluster}: {terms}\n")

import time
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Prepare the processed text
df['processed_text'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))

# Set max_features based on your previous experiments
max_features = 2000

# Function to apply K-means and evaluate with silhouette score
def apply_kmeans_and_evaluate(X, num_clusters, description):
    start_time = time.time()
    kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)
    kmeans.fit(X)
    end_time = time.time()

    labels = kmeans.labels_
    silhouette_avg = silhouette_score(X, labels)
    print(f"Silhouette Score for {num_clusters} clusters with {description}: {silhouette_avg}")
    print(f"Time taken for {description}: {end_time - start_time:.2f} seconds")
    return kmeans, labels

# Function to find optimal number of clusters
def find_optimal_clusters(X, max_k):
    silhouette_scores = []
    for k in range(2, max_k + 1):
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(X)
        score = silhouette_score(X, labels)
        silhouette_scores.append(score)
        print(f"Silhouette Score for {k} clusters: {score}")
    optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2
    return optimal_k

# TF-IDF Vectorization
start_time = time.time()
tfidf_vectorizer = TfidfVectorizer(max_features=max_features)
X_tfidf = tfidf_vectorizer.fit_transform(df['processed_text'])
end_time = time.time()
print(f"Time taken for TF-IDF vectorization: {end_time - start_time:.2f} seconds")

# BoW Vectorization
start_time = time.time()
bow_vectorizer = CountVectorizer(max_features=max_features)
X_bow = bow_vectorizer.fit_transform(df['processed_text'])
end_time = time.time()
print(f"Time taken for BoW vectorization: {end_time - start_time:.2f} seconds")

# Find optimal number of clusters for TF-IDF
optimal_k_tfidf = find_optimal_clusters(X_tfidf, 10)
print(f"Optimal number of clusters for TF-IDF: {optimal_k_tfidf}")

# Find optimal number of clusters for BoW
optimal_k_bow = find_optimal_clusters(X_bow, 10)
print(f"Optimal number of clusters for BoW: {optimal_k_bow}")

# Choose the number of PCA components
n_components = 100

# TF-IDF PCA and K-means
pca_tfidf = PCA(n_components=n_components)
X_tfidf_pca = pca_tfidf.fit_transform(X_tfidf.toarray())  # Convert sparse matrix to dense
kmeans_tfidf_pca, labels_tfidf_pca = apply_kmeans_and_evaluate(X_tfidf_pca, optimal_k_tfidf, 'TF-IDF')
df['tfidf_cluster_labels'] = labels_tfidf_pca  # Assign the cluster labels to your dataframe

# BoW PCA and K-means
pca_bow = PCA(n_components=n_components)
X_bow_pca = pca_bow.fit_transform(X_bow.toarray())  # Convert sparse matrix to dense
kmeans_bow_pca, labels_bow_pca = apply_kmeans_and_evaluate(X_bow_pca, optimal_k_bow, 'BoW')
df['bow_cluster_labels'] = labels_bow_pca  # Assign the cluster labels to your dataframe

# Now you have the time taken and cluster labels for each method

import time
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Prepare the processed text
df['processed_text'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))

# Set max_features based on your previous experiments
max_features = 2000

# Function to apply K-means and evaluate with silhouette score
def apply_kmeans_and_evaluate(X, num_clusters, description):
    start_time = time.time()
    kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)
    kmeans.fit(X)
    end_time = time.time()

    labels = kmeans.labels_
    silhouette_avg = silhouette_score(X, labels)
    print(f"Silhouette Score for {num_clusters} clusters with {description}: {silhouette_avg}")
    print(f"Time taken for {description}: {end_time - start_time:.2f} seconds")
    return kmeans, labels

# TF-IDF Vectorization
start_time = time.time()
tfidf_vectorizer = TfidfVectorizer(max_features=max_features)
X_tfidf = tfidf_vectorizer.fit_transform(df['processed_text'])
end_time = time.time()
print(f"Time taken for TF-IDF vectorization: {end_time - start_time:.2f} seconds")

# BoW Vectorization
start_time = time.time()
bow_vectorizer = CountVectorizer(max_features=max_features)
X_bow = bow_vectorizer.fit_transform(df['processed_text'])
end_time = time.time()
print(f"Time taken for BoW vectorization: {end_time - start_time:.2f} seconds")

# Choose the number of components and clusters
n_components = 100
num_clusters = 3

# TF-IDF PCA and K-means
pca_tfidf = PCA(n_components=n_components)
X_tfidf_pca = pca_tfidf.fit_transform(X_tfidf.toarray())  # Convert sparse matrix to dense
kmeans_tfidf_pca, labels_tfidf_pca = apply_kmeans_and_evaluate(X_tfidf_pca, num_clusters, 'TF-IDF')
df['tfidf_cluster_labels'] = labels_tfidf_pca  # Assign the cluster labels to your dataframe

# BoW PCA and K-means
pca_bow = PCA(n_components=n_components)
X_bow_pca = pca_bow.fit_transform(X_bow.toarray())  # Convert sparse matrix to dense
kmeans_bow_pca, labels_bow_pca = apply_kmeans_and_evaluate(X_bow_pca, num_clusters, 'BoW')
df['bow_cluster_labels'] = labels_bow_pca  # Assign the cluster labels to your dataframe

# Now you have the time taken and cluster labels for each method

from sklearn.metrics import silhouette_samples
import matplotlib.pyplot as plt

def simple_silhouette_analysis(X, labels):
    silhouette_vals = silhouette_samples(X, labels)
    plt.figure(figsize=(10, 7))

    # Plot silhouette score for each sample in a sorted manner
    y_ticks = []
    y_lower = 10
    for i, cluster in enumerate(np.unique(labels)):
        silhouette_vals_cluster = silhouette_vals[labels == cluster]
        silhouette_vals_cluster.sort()
        y_upper = y_lower + silhouette_vals_cluster.shape[0]
        plt.fill_betweenx(np.arange(y_lower, y_upper), 0, silhouette_vals_cluster)
        y_lower = y_upper + 10  # 10 for the 0 samples gap
        y_ticks += [(y_lower + y_upper) / 2]

    plt.yticks(y_ticks, np.unique(labels) + 1)
    plt.ylabel('Cluster')
    plt.xlabel('Silhouette coefficient')
    plt.title('Silhouette Plot')
    plt.show()

# Use the function with your PCA-reduced data and labels
simple_silhouette_analysis(X_tfidf_pca, labels_tfidf_pca)  # Replace with your actual data and labels

from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist
import time
import numpy as np

# K-Means clustering with TF-IDF
def kmeans_with_tfidf(X_train, num_clusters):
    tfidf_vectorizer = TfidfVectorizer(max_features=5000)
    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)

    kmeans = KMeans(n_clusters=num_clusters, random_state=42)

    # Start timing
    start_time_fit = time.time()

    kmeans.fit(X_train_tfidf)

    # End timing
    end_time_fit = time.time()

    fit_time = end_time_fit - start_time_fit

    # Evaluate clustering using silhouette score
    silhouette_avg = silhouette_score(X_train_tfidf, kmeans.labels_)
    print("Silhouette Score: {:.4f}".format(silhouette_avg))

    # Print Inertia and Distortion
    print("Inertia: {:.4f}".format(kmeans.inertia_))
    distortion = sum(np.min(cdist(X_train_tfidf.toarray(), kmeans.cluster_centers_, 'euclidean'), axis=1)) / X_train_tfidf.shape[0]
    print("Distortion: {:.4f}".format(distortion))

    return kmeans, tfidf_vectorizer, fit_time

# Choose the number of clusters
num_clusters_tfidf = 3

# Perform K-Means clustering with TF-IDF
kmeans_tfidf, tfidf_vectorizer, fit_time = kmeans_with_tfidf(X_train_str, num_clusters_tfidf)

# Print the fit time
print("Time taken for K-Means (fit): {:.2f} seconds".format(fit_time))

# Convert the test set to TF-IDF representation
X_test_tfidf = tfidf_vectorizer.transform(X_test_str)

# Start timing for predicting
start_time_predict = time.time()

# Assign clusters to the test set
test_labels_tfidf = kmeans_tfidf.predict(X_test_tfidf)

# End timing for predicting
end_time_predict = time.time()

predict_time = end_time_predict - start_time_predict

# Print the predict time
print("Time taken for K-Means (predict): {:.2f} seconds".format(predict_time))

from sklearn.feature_extraction.text import CountVectorizer

# K-Means clustering with Bag-of-Words
def kmeans_with_bow(X_train, num_clusters):
    bow_vectorizer = CountVectorizer(max_features=5000)
    X_train_bow = bow_vectorizer.fit_transform(X_train)

    kmeans = KMeans(n_clusters=num_clusters, random_state=42)

    # Start timing
    start_time_fit = time.time()

    kmeans.fit(X_train_bow)

    # End timing
    end_time_fit = time.time()

    fit_time = end_time_fit - start_time_fit

    # Evaluate clustering using silhouette score
    silhouette_avg = silhouette_score(X_train_bow, kmeans.labels_)
    print("Silhouette Score: {:.4f}".format(silhouette_avg))

    # Print Inertia and Distortion
    print("Inertia: {:.4f}".format(kmeans.inertia_))
    distortion = sum(np.min(cdist(X_train_bow.toarray(), kmeans.cluster_centers_, 'euclidean'), axis=1)) / X_train_bow.shape[0]
    print("Distortion: {:.4f}".format(distortion))

    return kmeans, bow_vectorizer, fit_time

# Perform K-Means clustering with Bag-of-Words
kmeans_bow, bow_vectorizer, fit_time_bow = kmeans_with_bow(X_train_str, num_clusters_tfidf)

# Print the fit time
print("Time taken for K-Means with Bag-of-Words (fit): {:.2f} seconds".format(fit_time_bow))

# Convert the test set to Bag-of-Words representation
X_test_bow = bow_vectorizer.transform(X_test_str)

# Start timing for predicting
start_time_predict_bow = time.time()

# Assign clusters to the test set
test_labels_bow = kmeans_bow.predict(X_test_bow)

# End timing for predicting
end_time_predict_bow = time.time()

predict_time_bow = end_time_predict_bow - start_time_predict_bow

# Print the predict time
print("Time taken for K-Means with Bag-of-Words (predict): {:.2f} seconds".format(predict_time_bow))